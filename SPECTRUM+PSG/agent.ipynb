{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name, instructions, model, handoffs=None):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.model = model\n",
    "        self.handoffs = handoffs or []\n",
    "        self.ai_proxy_url = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "        self.ai_proxy_token = os.getenv(\"AIPROXY_TOKEN\")  # Replace with your actual token\n",
    "    \n",
    "    def process(self, user_input):\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.ai_proxy_token}\"\n",
    "        }\n",
    "        \n",
    "        # Note: AI Proxy only supports gpt-4o-mini\n",
    "        if self.model != \"gpt-4o-mini\":\n",
    "            print(f\"Warning: Model {self.model} not supported. Using gpt-4o-mini instead.\")\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": self.instructions},\n",
    "                {\"role\": \"user\", \"content\": user_input}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.ai_proxy_url, headers=headers, json=data)\n",
    "        \n",
    "        # Check for headers with usage info\n",
    "        cost = response.headers.get(\"cost\", \"N/A\")\n",
    "        monthly_cost = response.headers.get(\"monthlyCost\", \"N/A\")\n",
    "        monthly_requests = response.headers.get(\"monthlyRequests\", \"N/A\")\n",
    "        \n",
    "        print(f\"Request cost: ${cost}\")\n",
    "        print(f\"Monthly usage: ${monthly_cost} ({monthly_requests} requests)\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    \n",
    "    def route(self, user_input):\n",
    "        if not self.handoffs:\n",
    "            return self.process(user_input)\n",
    "        \n",
    "        # For triage agent, first determine which agent to route to\n",
    "        routing_decision = self.process(f\"Route this request to the appropriate agent: {user_input}\")\n",
    "        \n",
    "        # Simple parsing of the triage response to determine which agent to use\n",
    "        for agent in self.handoffs:\n",
    "            if agent.name.lower() in routing_decision.lower():\n",
    "                print(f\"Routing to {agent.name}\")\n",
    "                return agent.process(user_input)\n",
    "        \n",
    "        # Default handling if no agent is identified\n",
    "        print(\"No specific agent identified, handling with triage agent\")\n",
    "        return self.process(user_input)\n",
    "\n",
    "# Initialize your agents\n",
    "booking_agent = Agent(\n",
    "    name=\"Booking Agent\", \n",
    "    instructions=\"Book tickets for a specific city and date\", \n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "refund_agent = Agent(\n",
    "    name=\"Refund Agent\", \n",
    "    instructions=\"Process refunds for tickets booked\", \n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\", \n",
    "    instructions=\"Triage the request and route it to the appropriate agent\", \n",
    "    model=\"gpt-4o-mini\", \n",
    "    handoffs=[booking_agent, refund_agent]\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = input(\"How can I help you today? \")\n",
    "    response = triage_agent.route(user_query)\n",
    "    print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Reasoning Explorer (type 'exit' to quit, 'debug' for debugging options)\n",
      "Available modes: normal, reasoning, chain, explore, perspectives, trace\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: Reasoning is the cognitive process of drawing conclusions, making inferences, or forming judgments based on available information or evidence. It involves the ability to think logically, analyze situations, and solve problems. There are several types of reasoning, including:\n",
      "\n",
      "1. **Deductive Reasoning**: This involves starting with a general statement or hypothesis and examining the possibilities to reach a specific, logical conclusion. For example, if all humans are mortal (general statement) and Socrates is a human, then Socrates is mortal (specific conclusion).\n",
      "\n",
      "2. **Inductive Reasoning**: This type of reasoning involves looking at specific instances or observations and making broader generalizations. For example, if you observe that the sun has risen in the east every day of your life, you might conclude that the sun always rises in the east.\n",
      "\n",
      "3. **Abductive Reasoning**: This is a form of reasoning that starts with an incomplete set of observations and proceeds to the likeliest possible explanation. For example, if you find the ground wet, you might conclude that it rained, although there could be other explanations (like someone watering the garden).\n",
      "\n",
      "4. **Analogical Reasoning**: This involves drawing comparisons between two similar situations or concepts to infer conclusions. For example, if you know that a certain medication works well for one illness, you might reason that it could also work for a similar illness.\n",
      "\n",
      "5. **Critical Thinking**: This is the ability to think clearly and rationally, understanding the logical connection between ideas. It involves evaluating arguments and identifying logical fallacies.\n",
      "\n",
      "Reasoning is essential in everyday decision-making, scientific inquiry, and philosophical discussions. It helps individuals navigate complex situations, understand relationships between concepts, and arrive at informed conclusions.\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: A Large Language Model (LLM) is a type of artificial intelligence model designed to understand, generate, and manipulate human language. These models are built using deep learning techniques, particularly neural networks, and are trained on vast amounts of text data from diverse sources, such as books, articles, websites, and more.\n",
      "\n",
      "Key characteristics of LLMs include:\n",
      "\n",
      "1. **Scale**: LLMs are characterized by their large number of parameters, often in the billions or even trillions. This scale allows them to capture complex patterns and nuances in language.\n",
      "\n",
      "2. **Training**: They are trained using unsupervised or semi-supervised learning methods, where the model learns to predict the next word in a sentence given the previous words. This process helps the model understand grammar, facts, and some level of reasoning.\n",
      "\n",
      "3. **Versatility**: LLMs can perform a wide range of language-related tasks, including text generation, translation, summarization, question answering, and more, often without needing task-specific training.\n",
      "\n",
      "4. **Contextual Understanding**: They can generate coherent and contextually relevant text by considering the context of the input they receive, which allows for more natural interactions.\n",
      "\n",
      "5. **Applications**: LLMs are used in various applications, including chatbots, virtual assistants, content creation, coding assistance, and more.\n",
      "\n",
      "Examples of well-known LLMs include OpenAI's GPT (Generative Pre-trained Transformer) series, Google's BERT (Bidirectional Encoder Representations from Transformers), and others. These models have significantly advanced the field of natural language processing (NLP) and have numerous practical applications across industries.\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: Yes, I am a language model (LLM) designed to assist with a variety of questions and tasks by generating human-like text based on the input I receive. How can I help you today?\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: No, I am not a Retrieval-Augmented Generation (RAG) model. I am a language model designed to generate text based on the input I receive, using the knowledge I was trained on up until October 2023. RAG models combine retrieval mechanisms with generative capabilities, allowing them to pull in external information to enhance their responses. In contrast, I generate responses based solely on the information and patterns learned during my training. If you have any questions or need assistance, feel free to ask!\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: I haven't been fine-tuned in the traditional sense because I operate based on a broad training dataset that allows me to generate responses across a wide range of topics. Fine-tuning typically involves additional training on a specific dataset to improve performance in a particular area or task. My design aims to provide general assistance and information rather than being specialized in one narrow field. However, I can still provide accurate and relevant information based on the knowledge I have. If you have specific needs or topics in mind, feel free to ask!\n",
      "Request cost: $N/A\n",
      "Monthly usage: $N/A (N/A requests)\n",
      "\n",
      "Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model = \"gpt-4o-mini\"):\n",
    "        self.model = model\n",
    "        self.ai_proxy_url = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "        self.ai_proxy_token = os.getenv(\"AIPROXY_TOKEN\", os.getenv(\"AIPROXY_TOKEN\"))  # Replace with your actual token\n",
    "        self.conversation_history = []\n",
    "        self.log_file = \"llm_interaction_logs.json\"\n",
    "\n",
    "    def _call_llm(self, messages: List[Dict[str, str]], temperature: float = 0) -> Dict[str, Any]:\n",
    "        \"\"\"Make a call to the LLM through the AI Proxy.\"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.ai_proxy_token}\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"gpt-4o-mini\",  # AI Proxy only supports this model\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.ai_proxy_url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Log usage data\n",
    "            cost = response.headers.get(\"cost\", \"N/A\")\n",
    "            monthly_cost = response.headers.get(\"monthlyCost\", \"N/A\")\n",
    "            monthly_requests = response.headers.get(\"monthlyRequests\", \"N/A\")\n",
    "            \n",
    "            print(f\"Request cost: ${cost}\")\n",
    "            print(f\"Monthly usage: ${monthly_cost} ({monthly_requests} requests)\")\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            # Add to conversation history\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "                \"request\": data,\n",
    "                \"response\": result,\n",
    "                \"usage_stats\": {\n",
    "                    \"cost\": cost,\n",
    "                    \"monthly_cost\": monthly_cost,\n",
    "                    \"monthly_requests\": monthly_requests\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # Save history periodically\n",
    "            if len(self.conversation_history) % 5 == 0:\n",
    "                self._save_history()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error calling LLM: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            return {\"error\": error_msg}\n",
    "\n",
    "    def _save_history(self):\n",
    "        \"\"\"Save conversation history to a file.\"\"\"\n",
    "        try:\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                json.dump(self.conversation_history, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving history: {str(e)}\")\n",
    "\n",
    "    def process(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\") -> str:\n",
    "        \"\"\"Process a user input with standard instructions.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        \n",
    "        result = self._call_llm(messages)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return f\"Error: {result['error']}\"\n",
    "        \n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def process_with_reasoning(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\") -> Dict[str, str]:\n",
    "        \"\"\"Process input and return both the response and reasoning.\"\"\"\n",
    "        reasoning_instructions = f\"{system_instructions}\\n\\nWhen answering, first explain your reasoning step by step, then provide your final answer.\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": reasoning_instructions},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        \n",
    "        result = self._call_llm(messages)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return {\"response\": f\"Error: {result['error']}\", \"reasoning\": \"\"}\n",
    "        \n",
    "        full_response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Try to separate reasoning from response\n",
    "        try:\n",
    "            # Simple parsing - can be improved for more structured outputs\n",
    "            if \"Reasoning:\" in full_response and \"Final Answer:\" in full_response:\n",
    "                reasoning, answer = full_response.split(\"Final Answer:\")\n",
    "                reasoning = reasoning.replace(\"Reasoning:\", \"\").strip()\n",
    "                answer = answer.strip()\n",
    "            else:\n",
    "                # Assume first 2/3 is reasoning, last 1/3 is answer\n",
    "                split_point = len(full_response) * 2 // 3\n",
    "                reasoning = full_response[:split_point].strip()\n",
    "                answer = full_response[split_point:].strip()\n",
    "                \n",
    "            return {\"response\": answer, \"reasoning\": reasoning}\n",
    "        except Exception:\n",
    "            # If parsing fails, return everything\n",
    "            return {\"response\": full_response, \"reasoning\": \"Could not separate reasoning from response\"}\n",
    "\n",
    "    def chain_of_thought(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\") -> Dict[str, str]:\n",
    "        \"\"\"Use explicit Chain of Thought prompting.\"\"\"\n",
    "        cot_prompt = f\"\"\"\n",
    "        {system_instructions}\n",
    "        \n",
    "        I want you to solve this step-by-step:\n",
    "        \n",
    "        {user_input}\n",
    "        \n",
    "        Please follow this format in your response:\n",
    "        \n",
    "        Step 1: [First step in your reasoning]\n",
    "        Step 2: [Second step in your reasoning]\n",
    "        ...\n",
    "        Final Answer: [Your final answer]\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You solve problems by showing your step-by-step reasoning before giving a final answer.\"},\n",
    "            {\"role\": \"user\", \"content\": cot_prompt}\n",
    "        ]\n",
    "        \n",
    "        result = self._call_llm(messages)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return {\"steps\": [], \"answer\": f\"Error: {result['error']}\"}\n",
    "        \n",
    "        full_response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Parse the steps and final answer\n",
    "        try:\n",
    "            steps = []\n",
    "            final_answer = \"\"\n",
    "            \n",
    "            lines = full_response.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"Step\"):\n",
    "                    step_content = line.split(\":\", 1)[1].strip() if \":\" in line else line\n",
    "                    steps.append(step_content)\n",
    "                elif line.startswith(\"Final Answer:\"):\n",
    "                    final_answer = line.replace(\"Final Answer:\", \"\").strip()\n",
    "            \n",
    "            if not final_answer and lines:\n",
    "                # If no explicit final answer, use the last line\n",
    "                final_answer = lines[-1]\n",
    "                \n",
    "            return {\"steps\": steps, \"answer\": final_answer}\n",
    "        except Exception as e:\n",
    "            return {\"steps\": [], \"answer\": full_response, \"parsing_error\": str(e)}\n",
    "\n",
    "    def explore_decision(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\", temperatures: List[float] = [0.0, 0.5, 1.0]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Explore different possible decisions by varying temperature.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        base_messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"{system_instructions}\\nExplain your reasoning thoroughly before giving your final answer.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            print(f\"Running with temperature {temp}...\")\n",
    "            result = self._call_llm(base_messages, temperature=temp)\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                response_text = f\"Error: {result['error']}\"\n",
    "            else:\n",
    "                response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                \n",
    "            results.append({\n",
    "                \"temperature\": temp,\n",
    "                \"response\": response_text\n",
    "            })\n",
    "            \n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def competing_perspectives(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\") -> Dict[str, str]:\n",
    "        \"\"\"Generate multiple perspectives on the same problem.\"\"\"\n",
    "        perspectives_prompt = f\"\"\"\n",
    "        {system_instructions}\n",
    "        \n",
    "        Consider this problem: {user_input}\n",
    "        \n",
    "        Please analyze this from multiple perspectives:\n",
    "        \n",
    "        Perspective 1: [First approach or viewpoint]\n",
    "        Reasoning: [Explain the reasoning for this perspective]\n",
    "        \n",
    "        Perspective 2: [Second approach or viewpoint]\n",
    "        Reasoning: [Explain the reasoning for this perspective]\n",
    "        \n",
    "        Perspective 3: [Third approach or viewpoint, if applicable]\n",
    "        Reasoning: [Explain the reasoning for this perspective]\n",
    "        \n",
    "        Analysis: [Compare the perspectives]\n",
    "        \n",
    "        Final Decision: [Your conclusion based on the analysis]\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": perspectives_prompt}\n",
    "        ]\n",
    "        \n",
    "        result = self._call_llm(messages)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return {\"perspectives\": [], \"decision\": f\"Error: {result['error']}\"}\n",
    "        \n",
    "        full_response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # Return the full structured response\n",
    "        return {\"full_analysis\": full_response}\n",
    "\n",
    "    def explicit_reasoning_trace(self, user_input: str, system_instructions: str = \"You are a helpful assistant.\") -> Dict[str, Any]:\n",
    "        \"\"\"Generate an explicit reasoning trace with factors and confidence.\"\"\"\n",
    "        trace_prompt = f\"\"\"\n",
    "        {system_instructions}\n",
    "        \n",
    "        For the following request, please provide a detailed reasoning trace in this format:\n",
    "        \n",
    "        QUESTION: {user_input}\n",
    "        \n",
    "        FACTORS TO CONSIDER:\n",
    "        - [List each relevant factor]\n",
    "        \n",
    "        REASONING TRACE:\n",
    "        1. [First step in analysis]\n",
    "        2. [Second step in analysis]\n",
    "        3. [Additional steps as needed]\n",
    "        \n",
    "        UNCERTAINTIES:\n",
    "        - [Any areas of uncertainty]\n",
    "        \n",
    "        CONFIDENCE: [Rate your confidence from 1-10]\n",
    "        \n",
    "        FINAL ANSWER: [Your conclusion]\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You provide detailed reasoning traces for complex questions.\"},\n",
    "            {\"role\": \"user\", \"content\": trace_prompt}\n",
    "        ]\n",
    "        \n",
    "        result = self._call_llm(messages)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            return {\"error\": result[\"error\"]}\n",
    "        \n",
    "        return {\"full_trace\": result[\"choices\"][0][\"message\"][\"content\"]}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the LLM client\n",
    "    llm_client = LLMClient()\n",
    "\n",
    "    # Interactive CLI\n",
    "    print(\"LLM Reasoning Explorer (type 'exit' to quit, 'debug' for debugging options)\")\n",
    "    print(\"Available modes: normal, reasoning, chain, explore, perspectives, trace\")\n",
    "    \n",
    "    mode = \"normal\"\n",
    "    system_instructions = \"You are a helpful, accurate, and thoughtful assistant.\"\n",
    "    \n",
    "    while True:\n",
    "        command = input(\"\\nMode [\" + mode + \"] > \")\n",
    "        \n",
    "        if command.lower() == 'exit':\n",
    "            break\n",
    "        elif command.lower() == 'debug':\n",
    "            print(\"\\nDebug Options:\")\n",
    "            print(\"1. Change mode\")\n",
    "            print(\"2. Set system instructions\")\n",
    "            print(\"3. View conversation history\")\n",
    "            \n",
    "            debug_choice = input(\"Select option: \")\n",
    "            if debug_choice == '1':\n",
    "                print(\"\\nAvailable modes:\")\n",
    "                print(\"- normal: Standard response\")\n",
    "                print(\"- reasoning: Response with reasoning explanation\")\n",
    "                print(\"- chain: Chain of thought reasoning\")\n",
    "                print(\"- explore: Explore different temperatures\")\n",
    "                print(\"- perspectives: Multiple competing perspectives\")\n",
    "                print(\"- trace: Explicit reasoning trace with confidence\")\n",
    "                \n",
    "                new_mode = input(\"Enter mode: \").lower()\n",
    "                if new_mode in ['normal', 'reasoning', 'chain', 'explore', 'perspectives', 'trace']:\n",
    "                    mode = new_mode\n",
    "                else:\n",
    "                    print(\"Invalid mode. Staying with current mode.\")\n",
    "            elif debug_choice == '2':\n",
    "                print(f\"\\nCurrent system instructions: {system_instructions}\")\n",
    "                new_instructions = input(\"Enter new system instructions (or press enter to keep current): \")\n",
    "                if new_instructions:\n",
    "                    system_instructions = new_instructions\n",
    "            elif debug_choice == '3':\n",
    "                print(f\"\\nConversation history ({len(llm_client.conversation_history)} entries):\")\n",
    "                for i, entry in enumerate(llm_client.conversation_history[-3:]):\n",
    "                    print(f\"Entry {i+1} - {entry['timestamp']}\")\n",
    "                    print(f\"Request: {entry['request']['messages'][-1]['content'][:50]}...\")\n",
    "                    if 'response' in entry and 'choices' in entry['response']:\n",
    "                        print(f\"Response: {entry['response']['choices'][0]['message']['content'][:50]}...\")\n",
    "                    print(f\"Cost: {entry['usage_stats']['cost']}\")\n",
    "                    print(\"-\" * 40)\n",
    "        else:\n",
    "            # Process the user query based on selected mode\n",
    "            if mode == \"normal\":\n",
    "                response = llm_client.process(command, system_instructions)\n",
    "                print(\"\\nResponse:\", response)\n",
    "            \n",
    "            elif mode == \"reasoning\":\n",
    "                result = llm_client.process_with_reasoning(command, system_instructions)\n",
    "                print(\"\\n=== REASONING ===\")\n",
    "                print(result[\"reasoning\"])\n",
    "                print(\"\\n=== FINAL RESPONSE ===\")\n",
    "                print(result[\"response\"])\n",
    "            \n",
    "            elif mode == \"chain\":\n",
    "                result = llm_client.chain_of_thought(command, system_instructions)\n",
    "                print(\"\\n=== REASONING STEPS ===\")\n",
    "                for i, step in enumerate(result[\"steps\"]):\n",
    "                    print(f\"Step {i+1}: {step}\")\n",
    "                print(\"\\n=== FINAL ANSWER ===\")\n",
    "                print(result[\"answer\"])\n",
    "            \n",
    "            elif mode == \"explore\":\n",
    "                results = llm_client.explore_decision(command, system_instructions, temperatures=[0.0, 0.7, 1.0])\n",
    "                for i, result in enumerate(results):\n",
    "                    print(f\"\\n=== TEMPERATURE {result['temperature']} ===\")\n",
    "                    print(result[\"response\"])\n",
    "            \n",
    "            elif mode == \"perspectives\":\n",
    "                result = llm_client.competing_perspectives(command, system_instructions)\n",
    "                print(\"\\n=== MULTIPLE PERSPECTIVES ANALYSIS ===\")\n",
    "                print(result[\"full_analysis\"])\n",
    "            \n",
    "            elif mode == \"trace\":\n",
    "                result = llm_client.explicit_reasoning_trace(command, system_instructions)\n",
    "                print(\"\\n=== REASONING TRACE ===\")\n",
    "                print(result[\"full_trace\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is the central component of Semantic Kernel. At its simplest, the kernel is a Dependency Injection container that manages all of the services and plugins necessary to run your AI application. If you provide all of your services and plugins to the kernel, they will then be seamlessly used by the AI as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatPromptExecutionSettings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion_service = OllamaChatCompletion(\n",
    "    ai_model_id=\"gemma3:4b\",\n",
    "    service_id=\"gemma3\", # Optional; for targeting specific services within Semantic Kernel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the chat completion service created above to the kernel\n",
    "kernel=Kernel()\n",
    "kernel.add_service(chat_completion_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the chat completion service by id\n",
    "chat_completion_service = kernel.get_service(service_id=\"gemma3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’m doing well, thank you for asking! As a large language model, I don’t really *feel* in the way humans do, but my systems are running smoothly and I’m ready to chat. 😊 \n",
      "\n",
      "How are *you* doing today? Is there anything you’d like to talk about, or maybe I can help you with something?"
     ]
    }
   ],
   "source": [
    "#streaming chat application\n",
    "\n",
    "chat_history = ChatHistory()\n",
    "chat_history.add_user_message(\"Hello, how are you?\")\n",
    "\n",
    "response = chat_completion_service.get_streaming_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=OllamaChatPromptExecutionSettings()\n",
    ")\n",
    "\n",
    "async for chunk in response:\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main agents using agno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.tools.googlesearch import GoogleSearchTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 10 features of GetStream MultiAgent AI Frameworks:\n",
      "1. Extensible and customizable architecture\n",
      "2. Open-source libraries and tools\n",
      "3. Strong support for various programming languages\n",
      "4. Extensive documentation and community support\n",
      "5. Integration with popular frameworks like Spring, React, etc.\n",
      "6. Scalability to handle large datasets and complex simulations\n",
      "7. Advanced machine learning capabilities\n",
      "8. Real-time data processing and analysis\n",
      "9. Data privacy and security measures in place\n",
      "10. Comprehensive set of features for multiagent system development\n"
     ]
    }
   ],
   "source": [
    "#while running an agent dont forget to give description and instructions to it a sample webscrapping agent\n",
    "agent = Agent(\n",
    "    model=Ollama(\"nemotron-mini:4b\"),\n",
    "    tools=[\n",
    "        GoogleSearchTools(),\n",
    "    ],\n",
    "    description=\"you are an helpful web scrapper who helps me scrape data from any website.\",\n",
    "    instructions=[\n",
    "        \"Given a link by the user, respond with the top 10 highlighted features scrapped from the website\"\n",
    "        \".\",\n",
    "    ],\n",
    "    show_tool_calls=True\n",
    ")\n",
    "response=agent.run(\"https://getstream.io/blog/multiagent-ai-frameworks/\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama\n",
    "from agno.tools.googlesearch import GoogleSearchTools\n",
    "from agno.tools.yfinance import YFinanceTools\n",
    "from agno.tools.website import WebsiteTools\n",
    "import asyncio\n",
    "import nest_asyncio  # For web scraping\n",
    "\n",
    "nest_asyncio.apply()  # Apply the patch to allow nested event loops\n",
    "\n",
    "# Web scraping agent\n",
    "web_agent = Agent(\n",
    "    name=\"Web Scraper\",\n",
    "    role=\"Scrape data from websites\",\n",
    "    model=Ollama(\"nemotron-mini:4b\"),\n",
    "    tools=[\n",
    "        WebsiteTools(),  # Use WebTools for scraping instead of GoogleSearchTools\n",
    "        GoogleSearchTools()  # Keep this for searching\n",
    "    ],\n",
    "    description=\"You are a helpful web scraper who extracts data from websites.\",\n",
    "    instructions=[\n",
    "        \"Given a link by the user, extract the relevant information from the website.\",\n",
    "        \"Use the web scraping tool to get the content of web pages.\",\n",
    "        \"Use search when you need to find relevant websites first.\"\n",
    "    ],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True\n",
    ")\n",
    "\n",
    "# Finance agent\n",
    "finance_agent = Agent(\n",
    "    name=\"Finance Agent\",\n",
    "    role=\"Get financial data\",\n",
    "    model=Ollama(\"nemotron-mini:4b\"),\n",
    "    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],\n",
    "    description=\"You are a finance expert who provides financial data and analysis.\",\n",
    "    instructions=[\n",
    "        \"Use tables to display financial data.\",\n",
    "        \"Provide clear explanations of financial metrics.\",\n",
    "        \"Always cite the source of your financial data.\"\n",
    "    ],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True\n",
    ")\n",
    "\n",
    "# Team agent\n",
    "agent_team = Agent(\n",
    "    team=[web_agent, finance_agent],\n",
    "    model=Ollama(\"nemotron-mini:4b\"),\n",
    "    description=\"You are a research assistant that combines web scraping and financial analysis capabilities.\",\n",
    "    instructions=[\n",
    "        \"Use the Finance Agent to get financial data from yfinance.\",\n",
    "        \"Use the Web Scraper to find and extract relevant information from websites.\",\n",
    "        \"Always include sources for your information.\",\n",
    "        \"Use tables to display structured data.\",\n",
    "        \"Provide a comprehensive analysis that combines financial data with market insights.\"\n",
    "    ],\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,debug_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid escape sequence '\\A'\n",
      "invalid escape sequence '\\A'\n",
      "invalid escape sequence '\\A'\n",
      "WARNING:root:Pretrained weights (C:\\Users\\rexjo/.cache/autodistill/open_clip/b32_400m.pt) not found for model ViT-B-32-quickgelu. Available pretrained tags (['openai', 'laion400m_e31', 'laion400m_e32', 'metaclip_400m', 'metaclip_fullcc'].\n",
      "invalid escape sequence '\\A'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pretrained weights (C:\\Users\\rexjo/.cache/autodistill/open_clip/b32_400m.pt) not found for model ViT-B-32-quickgelu. Available pretrained tags (['openai', 'laion400m_e31', 'laion400m_e32', 'metaclip_400m', 'metaclip_fullcc'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define an ontology to map class names to our MetaCLIP prompt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# the ontology dictionary has the format {caption: class}\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# where caption is the prompt sent to the base model, and class is the label that will\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# be saved for that caption in the generated annotations\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# then, load the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m base_model \u001b[38;5;241m=\u001b[39m MetaCLIP(\n\u001b[0;32m     11\u001b[0m     ontology\u001b[38;5;241m=\u001b[39mCaptionOntology(\n\u001b[0;32m     12\u001b[0m         {\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma forklift\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforklift\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m         }\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Specify the correct pretrained tag\u001b[39;00m\n\u001b[0;32m     20\u001b[0m pretrained_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaion400m_e31\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use a valid pretrained tag\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rexjo\\anaconda3\\Lib\\site-packages\\autodistill_metaclip\\metaclip_model.py:27\u001b[0m, in \u001b[0;36mMetaCLIP.__init__\u001b[1;34m(self, ontology)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ontology: CaptionOntology):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess \u001b[38;5;241m=\u001b[39m open_clip\u001b[38;5;241m.\u001b[39mcreate_model_and_transforms(\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B-32-quickgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m         pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/.cache/autodistill/open_clip/b32_400m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39montology \u001b[38;5;241m=\u001b[39m ontology\n",
      "File \u001b[1;32mc:\\Users\\rexjo\\anaconda3\\Lib\\site-packages\\open_clip\\factory.py:502\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, load_weights_only, **model_kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model_and_transforms\u001b[39m(\n\u001b[0;32m    473\u001b[0m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    474\u001b[0m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    493\u001b[0m ):\n\u001b[0;32m    494\u001b[0m     force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[0;32m    495\u001b[0m         {},\n\u001b[0;32m    496\u001b[0m         mean\u001b[38;5;241m=\u001b[39mimage_mean,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    499\u001b[0m         resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[1;32m--> 502\u001b[0m     model \u001b[38;5;241m=\u001b[39m create_model(\n\u001b[0;32m    503\u001b[0m         model_name,\n\u001b[0;32m    504\u001b[0m         pretrained,\n\u001b[0;32m    505\u001b[0m         precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[0;32m    506\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    507\u001b[0m         jit\u001b[38;5;241m=\u001b[39mjit,\n\u001b[0;32m    508\u001b[0m         force_quick_gelu\u001b[38;5;241m=\u001b[39mforce_quick_gelu,\n\u001b[0;32m    509\u001b[0m         force_custom_text\u001b[38;5;241m=\u001b[39mforce_custom_text,\n\u001b[0;32m    510\u001b[0m         force_patch_dropout\u001b[38;5;241m=\u001b[39mforce_patch_dropout,\n\u001b[0;32m    511\u001b[0m         force_image_size\u001b[38;5;241m=\u001b[39mforce_image_size,\n\u001b[0;32m    512\u001b[0m         force_preprocess_cfg\u001b[38;5;241m=\u001b[39mforce_preprocess_cfg,\n\u001b[0;32m    513\u001b[0m         pretrained_image\u001b[38;5;241m=\u001b[39mpretrained_image,\n\u001b[0;32m    514\u001b[0m         pretrained_hf\u001b[38;5;241m=\u001b[39mpretrained_hf,\n\u001b[0;32m    515\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    516\u001b[0m         output_dict\u001b[38;5;241m=\u001b[39moutput_dict,\n\u001b[0;32m    517\u001b[0m         load_weights_only\u001b[38;5;241m=\u001b[39mload_weights_only,\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m     pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[0;32m    523\u001b[0m     preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[0;32m    524\u001b[0m         pp_cfg,\n\u001b[0;32m    525\u001b[0m         is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    526\u001b[0m         aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[0;32m    527\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rexjo\\anaconda3\\Lib\\site-packages\\open_clip\\factory.py:406\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, load_weights_only, **model_kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m         error_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPretrained weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not found for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    404\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Available pretrained tags (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_pretrained_tags_by_model(model_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    405\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(error_str)\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_str)\n\u001b[0;32m    407\u001b[0m     pretrained_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_hf_hub_prefix:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Pretrained weights (C:\\Users\\rexjo/.cache/autodistill/open_clip/b32_400m.pt) not found for model ViT-B-32-quickgelu. Available pretrained tags (['openai', 'laion400m_e31', 'laion400m_e32', 'metaclip_400m', 'metaclip_fullcc']."
     ]
    }
   ],
   "source": [
    "from autodistill_metaclip import MetaCLIP\n",
    "from autodistill.detection import CaptionOntology\n",
    "import os\n",
    "\n",
    "# Define an ontology to map class names to our MetaCLIP prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = MetaCLIP(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"person\": \"person\",\n",
    "            \"a forklift\": \"forklift\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Specify the correct pretrained tag\n",
    "pretrained_tag = \"laion400m_e31\"  # Use a valid pretrained tag\n",
    "\n",
    "# Load the model with the correct pretrained tag\n",
    "base_model = MetaCLIP(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"person\": \"person\",\n",
    "            \"a forklift\": \"forklift\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "results = base_model.predict(\"C:\\AI_PROJECTS\\Vitstuff\\WhatsApp Image 2025-03-07 at 23.26.04_1e48c753.jpg\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
